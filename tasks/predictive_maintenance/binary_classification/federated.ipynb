{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../datasets/Machine Predictive Maintenance Classification/binary_classification.csv\", index_col=[0])\n",
    "\n",
    "X = df.drop(columns='Target')\n",
    "Y = df['Target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.reset_index(drop=True)  # Reset indices to avoid indexing issues\n",
    "        self.y = y.reset_index(drop=True)  # Reset indices to avoid indexing issues\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            X_tensor = torch.tensor(self.X.iloc[idx].values, dtype=torch.float32)\n",
    "            y_tensor = torch.tensor(self.y.iloc[idx], dtype=torch.long)\n",
    "            return X_tensor, y_tensor\n",
    "        except TypeError:\n",
    "            self._check_indexing_error(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}, Index: {idx}\")\n",
    "\n",
    "    def _check_indexing_error(self, idx):\n",
    "        if isinstance(idx, (list, tuple, pd.Index)):\n",
    "            raise IndexError(\"Invalid index provided. Index should be an integer.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(x_train, y_train)\n",
    "test_dataset = CustomDataset(x_test, y_test)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train_dataset into 5 subsets\n",
    "num_subsets = 5\n",
    "subset_size = len(train_dataset) // num_subsets\n",
    "indices = list(range(len(train_dataset)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "subsets = []\n",
    "for i in range(num_subsets):\n",
    "    start_idx = i * subset_size\n",
    "    end_idx = (i + 1) * subset_size if i != num_subsets - 1 else len(train_dataset)\n",
    "    subset_indices = indices[start_idx:end_idx]\n",
    "    subsets.append(Subset(train_dataset, subset_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.9839, 0.9847, 0.6015, 0.3407, 0.5771]), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "print(subsets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "input_size = X.shape[1]\n",
    "hidden_size = 10\n",
    "num_classes = 2\n",
    "\n",
    "# model = SimpleNN(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, optimizer, epochs):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    net.train()\n",
    "    for _ in range(epochs):\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return net\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, loss = 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def run_model(epochs: int, lr: float, model: SimpleNN, weights_list: list, train_dataset: CustomDataset, test_dataset: CustomDataset, momentum: float = 0.9):\n",
    "# def run_centralised(epochs: int, lr: float, momentum: float = 0.9):\n",
    "    # instantiate the model\n",
    "    # model = SimpleNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "    # define optimiser with hyperparameters supplied\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    # train for the specified number of epochs\n",
    "    trained_model = train(model, train_loader, optim, epochs)\n",
    "    weights_list.append(trained_model.state_dict())\n",
    "    # training is completed, then evaluate model on the test set\n",
    "    loss, accuracy = test(trained_model, test_loader)\n",
    "    print(f\"{loss = }\")\n",
    "    print(f\"{accuracy = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_2 =  model_3 = model_4 = model_5 = SimpleNN(input_size, hidden_size, num_classes)\n",
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "trained_weights = []\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "loss = 97.09507640550146\n",
      "accuracy = 0.97\n",
      "Model 2\n",
      "loss = 145.14154269965366\n",
      "accuracy = 0.964\n",
      "Model 3\n",
      "loss = 92.94379867471434\n",
      "accuracy = 0.971\n",
      "Model 4\n",
      "loss = 95.06539203775174\n",
      "accuracy = 0.971\n",
      "Model 5\n",
      "loss = 92.89945761350646\n",
      "accuracy = 0.971\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_subsets):\n",
    "    print(f\"Model {i+1}\")\n",
    "    run_model(epochs=5, lr=0.01, model=models[i], weights_list=trained_weights, train_dataset=subsets[i], test_dataset=test_dataset )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.2807,  0.2276, -0.6451,  2.4638, -3.6933],\n",
       "                      [-0.3303, -0.0611,  0.4120, -0.1704, -0.1221],\n",
       "                      [-0.6206, -1.3491,  7.5297, -5.9153, -0.8620],\n",
       "                      [-0.3202,  0.1524,  0.3599, -0.2376, -0.2158],\n",
       "                      [ 1.7856,  3.3416, -0.5304, -6.8118, -3.2449],\n",
       "                      [-0.2504, -0.4203, -0.3091, -0.3718, -0.1661],\n",
       "                      [-0.2449, -0.0690, -0.1188, -0.3392,  0.4416],\n",
       "                      [ 0.0296, -0.4196,  0.2972, -0.2722, -0.0307],\n",
       "                      [-0.2273, -0.1861,  0.0682,  0.1026, -0.1625],\n",
       "                      [ 0.6068,  0.7047, -0.1529, -1.2843, -0.6664]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.2508, -0.0386, -1.3906, -0.2112,  3.4596, -0.0428, -0.2679, -0.0641,\n",
       "                      -0.1146,  0.3942])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.9469,  0.2794, -2.3889, -0.2242,  0.8382, -0.2219,  0.0641,  0.0818,\n",
       "                       -0.0258,  0.1645],\n",
       "                      [ 0.5247,  0.0934,  2.1071,  0.3121, -1.1183, -0.0800,  0.1045,  0.1760,\n",
       "                       -0.2906, -0.2143]])),\n",
       "             ('fc2.bias', tensor([-0.0661, -0.1137]))])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_model_weights(weight_list):\n",
    "    \"\"\"\n",
    "    Average the weights of a list of models with the same architecture.\n",
    "\n",
    "    Args:\n",
    "        models (list): List of PyTorch models with the same architecture.\n",
    "\n",
    "    Returns:\n",
    "        dict: Averaged model weights.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the averaged weights\n",
    "    avg_weights = {}\n",
    "\n",
    "    # Get the state_dict of the first model as a template\n",
    "    first_model_state_dict = weight_list[0]\n",
    "    \n",
    "    # Initialize the avg_weights with the structure of the first model\n",
    "    for key in first_model_state_dict.keys():\n",
    "        avg_weights[key] = torch.zeros_like(first_model_state_dict[key])\n",
    "    \n",
    "    # Iterate through each model and accumulate the weights\n",
    "    for weight in weight_list:\n",
    "        state_dict = weight\n",
    "        for key in state_dict.keys():\n",
    "            avg_weights[key] += state_dict[key]\n",
    "    \n",
    "    # Divide each weight by the number of models to get the average\n",
    "    num_models = len(weight_list)\n",
    "    for key in avg_weights.keys():\n",
    "        avg_weights[key] /= num_models\n",
    "    \n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_weights = average_model_weights(weight_list=trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = SimpleNN(input_size, hidden_size, num_classes)  # Create the model with the same architecture\n",
    "new_model.load_state_dict(averaged_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.2807,  0.2276, -0.6451,  2.4638, -3.6933],\n",
       "                      [-0.3303, -0.0611,  0.4120, -0.1704, -0.1221],\n",
       "                      [-0.6206, -1.3491,  7.5297, -5.9153, -0.8620],\n",
       "                      [-0.3202,  0.1524,  0.3599, -0.2376, -0.2158],\n",
       "                      [ 1.7856,  3.3416, -0.5304, -6.8118, -3.2449],\n",
       "                      [-0.2504, -0.4203, -0.3091, -0.3718, -0.1661],\n",
       "                      [-0.2449, -0.0690, -0.1188, -0.3392,  0.4416],\n",
       "                      [ 0.0296, -0.4196,  0.2972, -0.2722, -0.0307],\n",
       "                      [-0.2273, -0.1861,  0.0682,  0.1026, -0.1625],\n",
       "                      [ 0.6068,  0.7047, -0.1529, -1.2843, -0.6664]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.2508, -0.0386, -1.3906, -0.2112,  3.4596, -0.0428, -0.2679, -0.0641,\n",
       "                      -0.1146,  0.3942])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.9469,  0.2794, -2.3889, -0.2242,  0.8382, -0.2219,  0.0641,  0.0818,\n",
       "                       -0.0258,  0.1645],\n",
       "                      [ 0.5247,  0.0934,  2.1071,  0.3121, -1.1183, -0.0800,  0.1045,  0.1760,\n",
       "                       -0.2906, -0.2143]])),\n",
       "             ('fc2.bias', tensor([-0.0661, -0.1137]))])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 92.89945761350646\n",
      "accuracy = 0.971\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = test(model_1, DataLoader(test_dataset, batch_size=2, shuffle=False))\n",
    "print(f\"{loss = }\")\n",
    "print(f\"{accuracy = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed-learning",
   "language": "python",
   "name": "fed-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
