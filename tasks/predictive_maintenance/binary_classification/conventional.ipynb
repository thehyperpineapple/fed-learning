{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../datasets/Machine Predictive Maintenance Classification/binary_classification.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Target')\n",
    "Y = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"Using GPU: {device_name}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.reset_index(drop=True)  # Reset indices to avoid indexing issues\n",
    "        self.y = y.reset_index(drop=True)  # Reset indices to avoid indexing issues\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            X_tensor = torch.tensor(self.X.iloc[idx].values, dtype=torch.float32)\n",
    "            y_tensor = torch.tensor(self.y.iloc[idx], dtype=torch.long)\n",
    "            return X_tensor, y_tensor\n",
    "        except TypeError:\n",
    "            self._check_indexing_error(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}, Index: {idx}\")\n",
    "\n",
    "    def _check_indexing_error(self, idx):\n",
    "        if isinstance(idx, (list, tuple, pd.Index)):\n",
    "            raise IndexError(\"Invalid index provided. Index should be an integer.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(x_train, y_train)\n",
    "test_dataset = CustomDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "input_size = X.shape[1]\n",
    "hidden_size = 10\n",
    "num_classes = 2\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters = 82\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum(value.numel() for value in model.state_dict().values())\n",
    "print(f\"{num_parameters = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, optimizer, epochs):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    net.train()\n",
    "    for _ in range(epochs):\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return net\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, loss = 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def run_centralised(epochs: int, lr: float, model: SimpleNN, train_dataset: CustomDataset, test_dataset: CustomDataset, momentum: float = 0.9):\n",
    "# def run_centralised(epochs: int, lr: float, momentum: float = 0.9):\n",
    "    # instantiate the model\n",
    "    # model = SimpleNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "    # define optimiser with hyperparameters supplied\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    train_dataset = CustomDataset(x_train, y_train)\n",
    "    test_dataset = CustomDataset(x_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    # train for the specified number of epochs\n",
    "    trained_model = train(model, train_loader, optim, epochs)\n",
    "\n",
    "    # training is completed, then evaluate model on the test set\n",
    "    loss, accuracy = test(trained_model, test_loader)\n",
    "    print(f\"{loss = }\")\n",
    "    print(f\"{accuracy = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 100.25208443822339\n",
      "accuracy = 0.972\n"
     ]
    }
   ],
   "source": [
    "run_centralised(epochs=5, lr=0.01, model=model, train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "# run_centralised(epochs=5, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 7.2073e-01,  1.1847e+00, -6.9887e-01, -2.4762e+00, -8.6059e-01],\n",
       "                      [-8.6437e-01, -1.7447e-01, -1.4074e+00,  3.8150e+00,  8.0337e-01],\n",
       "                      [-7.7218e-02,  1.1339e-01,  1.5609e+00, -1.2647e+00, -1.1231e-02],\n",
       "                      [ 7.0153e-01,  1.0174e+00, -9.8110e-01, -2.1112e+00, -6.0228e-01],\n",
       "                      [-7.6304e-02,  4.8556e-01, -2.0388e-01, -1.9905e-01, -2.4395e-01],\n",
       "                      [-3.9132e-01,  3.9288e-01, -2.4111e-01, -3.5332e-01, -2.7727e-01],\n",
       "                      [ 1.5035e-01, -5.6717e-02, -1.9578e-01, -1.8157e-01,  5.3484e-02],\n",
       "                      [-1.3667e-01, -2.4926e-01, -1.1325e-01,  8.5793e-02, -4.1751e-01],\n",
       "                      [-3.4502e-01,  9.9287e-02,  2.5702e-01, -1.7292e-03, -4.3818e-01],\n",
       "                      [ 6.4855e-03,  7.4033e-01, -4.0296e-01, -1.1668e+00, -4.5637e-01]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 1.2658, -0.8143, -0.5112,  1.0927,  0.0831,  0.2558,  0.1468,  0.3484,\n",
       "                      -0.0392,  0.8466])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.4535, -0.9319, -1.1363,  0.6877,  0.2074, -0.2941,  0.3112, -0.2772,\n",
       "                       -0.1065,  0.4312],\n",
       "                      [-0.5052,  1.1568,  1.1252, -0.1777,  0.0189,  0.1519,  0.2505,  0.0539,\n",
       "                        0.0176, -0.0794]])),\n",
       "             ('fc2.bias', tensor([ 1.6699, -1.2617]))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# This function partitions the training set into N disjoint subsets, each will become the local dataset of a client. This function also subsequently partitions each traininset partition into train and validation. The test set is left intact and will be used by the central server to asses the performance of the global model.\n",
    "def prepare_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1):\n",
    "    train_dataset = CustomDataset(x_train, y_train)\n",
    "    test_dataset = CustomDataset(x_test, y_test)\n",
    "\n",
    "    num_instances = len(train_dataset) // num_partitions\n",
    "    partition_len = [num_instances] * num_partitions\n",
    "\n",
    "    trainsets = random_split(\n",
    "        train_dataset, partition_len, torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    #Creating Dataloaders with train and validation support\n",
    "\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for trainset_ in trainsets:\n",
    "        num_total = len(trainset_)\n",
    "        num_val = int(val_ratio * num_total)\n",
    "        num_train = num_total - num_val\n",
    "\n",
    "        for_train, for_val = random_split(\n",
    "            trainset_, [num_train, num_val], torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        trainloaders.append(\n",
    "            DataLoader(for_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        )\n",
    "        valloaders.append(\n",
    "            DataLoader(for_val, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "            \n",
    "        )\n",
    "    testloader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloaders, valloaders, testloader = prepare_dataset(\n",
    "    num_partitions=10, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first partition\n",
    "len(trainloaders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed-learning",
   "language": "python",
   "name": "fed-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
